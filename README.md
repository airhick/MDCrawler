# Advanced Crawler Webhook v2.0

Service FastAPI exposant un webhook `/crawl` qui prend une URL, explore en largeur (depth r√©glable), et renvoie un contenu structur√© et nettoy√© en Markdown avec m√©tadonn√©es enrichies, optimis√© pour les LLM et l'analyse de donn√©es.

## Nouveaut√©s v2.0 üöÄ

### Structure Globale
- ‚úÖ YAML front-matter par page (url, crawled_at, page_type, lang, content_hash)
- ‚úÖ Hi√©rarchie de titres normalis√©e (H1 > H2 > H3 coh√©rente)
- ‚úÖ D√©duplication automatique des blocs de contenu r√©p√©t√©s
- ‚úÖ Ancres automatiques sur tous les titres
- ‚úÖ Filtrage des titres marketing vides ("√Ä LA UNE", "NOS SERVICES", etc.)

### Donn√©es Structur√©es
- ‚úÖ D√©tection automatique du type de page (home, store, product, category, service, faq, contact, about)
- ‚úÖ D√©tection de la langue (fr, de, it, en) via HTML, URL et contenu
- ‚úÖ Extraction et normalisation des coordonn√©es:
  - Emails avec liens `mailto:`
  - T√©l√©phones normalis√©s E.164 avec liens `tel:`
  - Adresses d√©tect√©es et normalis√©es
- ‚úÖ Horaires d'ouverture pars√©s en tableaux Markdown
- ‚úÖ URLs canoniques extraites
- ‚úÖ Hash de contenu pour d√©duplication

### Nettoyage & Normalisation
- ‚úÖ Normalisation des espaces, quotes, apostrophes
- ‚úÖ Formatage coh√©rent des devises (CHF, EUR, USD)
- ‚úÖ Capitalisation correcte des noms de villes suisses
- ‚úÖ Suppression des CTAs dupliqu√©s et slogans isol√©s
- ‚úÖ Fusion des phrases monosyllabiques isol√©es

### Rapport de Crawl Enrichi
- ‚úÖ Statistiques compl√®tes (pages, types, langues, doublons)
- ‚úÖ Version du crawler et User-Agent utilis√©
- ‚úÖ Horodatage ISO 8601
- ‚úÖ Compteurs par type (magasins, produits, cat√©gories trouv√©s)
- ‚úÖ Chaque page marqu√©e clairement (PAGE 1, PAGE 2, etc.)
- ‚úÖ URL source en pied de chaque page

## Installation

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

## Lancement

```bash
uvicorn app:app --host 0.0.0.0 --port 8080
```

### Docker (d√©ploiement ou ex√©cution serveur)

Build et run en exposant le port publiquement:

```bash
docker build -t crawler-webhook \
  -f "/Users/Eric.AELLEN/Documents/A - projets pro/crawler/1.0/Dockerfile" \
  "/Users/Eric.AELLEN/Documents/A - projets pro/crawler/1.0"

docker run --rm -p 8080:8080 --name crawler-webhook crawler-webhook
```

Vous pouvez ensuite appeler `http://<ip_serveur>:8080/crawl` depuis l'ext√©rieur.

### Render (h√©bergement manag√©)

1. Poussez ce dossier dans un repo Git (GitHub/GitLab).
2. Sur Render, cr√©ez un nouveau Web Service, connectez le repo.
3. Render d√©tectera `render.yaml` et provisionnera automatiquement.
4. Une URL publique sera fournie, ex: `https://simple-crawler-webhook.onrender.com/crawl`.

### Railway (h√©bergement manag√©)

1. Installez l'outil CLI Railway: `npm i -g @railway/cli`.
2. Depuis ce dossier: `railway init` (ou `railway up` pour cr√©er le service).
3. Railway d√©tectera `railway.toml`. Aucune config additionnelle requise.
4. Le service d√©marre avec: `uvicorn app:app --host 0.0.0.0 --port $PORT`.
5. Une URL publique sera fournie, ex: `https://<projet>-up.railway.app/crawl`.

## Utilisation

- GET:
```bash
curl --get \
  --data-urlencode "url=https://example.com" \
  --data-urlencode "depth=1" \
  --data-urlencode "max_pages=5" \
  --data-urlencode "same_domain=true" \
  --data-urlencode "use_sitemap=true" \
  --data-urlencode "timeout=15" \
  --data-urlencode "max_chars_per_page=15000" \
  --data-urlencode "rate_limit_delay=0.2" \
  http://localhost:8080/crawl
```

- POST (JSON):
```bash
curl -X POST http://localhost:8080/crawl \
  -H 'Content-Type: application/json' \
  -d '{
    "url": "https://example.com",
    "depth": 1,
    "max_pages": 5,
    "same_domain": true,
    "use_sitemap": true,
    "sitemap_url": null,
    "sitemap_max_urls": 200,
    "timeout": 15,
    "max_chars_per_page": 15000,
    "rate_limit_delay": 0.2,
    "exclude_patterns": ["/login", "\\.pdf$"]
  }'
```

R√©ponse: `text/markdown`.

## Param√®tres

- `url` (obligatoire): URL de d√©part.
- `depth` (0-5): profondeur BFS (0 = seulement la page de d√©part).
- `max_pages` (1-200): nombre maximum de pages √† extraire.
- `same_domain` (bool): restreindre au m√™me domaine.
- `timeout` (1-60s): timeout HTTP par requ√™te.
- `max_chars_per_page` (1000-200000): limite de caract√®res extraits par page.
- `rate_limit_delay` (0-5s): d√©lai entre les requ√™tes.
- `exclude_patterns` (liste de regex): URLs √† ignorer.
- `user_agent` (POST): UA custom.
- `use_sitemap` (bool): activer la d√©couverte via sitemap.
- `sitemap_url` (str|null): URL sitemap explicite (sinon robots.txt + /sitemap.xml).
- `sitemap_max_urls` (int): limite d'URLs import√©es depuis les sitemaps.

## Format de Sortie

Chaque page crawl√©e inclut:

```markdown
================================================================================
PAGE 1
================================================================================

‚Äã```yaml
---
url: https://example.com/page
canonical_url: https://example.com/canonical
crawled_at: 2025-10-15T12:34:56Z
page_type: store
lang: fr
content_hash: abc123def456...
---
‚Äã```

## Titre de la Page

**URL**: [https://example.com/page](https://example.com/page)

**Description**: Description meta de la page

### Contact Information

**Email**:
- [contact@example.com](mailto:contact@example.com)

**T√©l√©phone**:
- [+41 22 788 00 22](tel:+41227880022)

**Adresse**:
- Route de Pr√©-Bois 14, 1216 Cointrin

**Horaires**:

| Jour | Horaires |
|------|----------|
| Lundi | Ferm√© |
| Mardi | 09:00 - 12:30, 13:30 - 18:30 |
| Mercredi | 09:00 - 12:30, 13:30 - 18:30 |
...

### Content

[Contenu nettoy√© et structur√© de la page...]

---
*Source: https://example.com/page*
```

## Types de Pages D√©tect√©s

Le crawler identifie automatiquement les types suivants:
- **home**: Page d'accueil
- **store**: Page de magasin/point de vente
- **product**: Fiche produit individuelle
- **category**: Page de cat√©gorie/catalogue
- **service**: Page de service/prestation
- **faq**: Page FAQ
- **contact**: Page de contact
- **about**: Page "√Ä propos"
- **page**: Page g√©n√©rique

## Langues D√©tect√©es

D√©tection automatique via:
1. Attribut HTML `lang`
2. Patterns dans l'URL (`/fr/`, `/de/`, etc.)
3. Analyse heuristique du contenu

Langues support√©es: `fr`, `de`, `it`, `en`, `unknown`

## Notes Techniques

- Extraction intelligente: titres, paragraphes, listes, blocs code, citations
- Nettoyage avanc√©: suppression scripts, styles, nav, footer, iframe, formulaires
- Normalisation hi√©rarchique des titres (√©vite les sauts de niveaux)
- D√©duplication bas√©e sur hash MD5 du contenu normalis√©
- Support des caract√®res sp√©ciaux et accents fran√ßais/allemand
- Normalisation t√©l√©phone E.164 pour Suisse (+41)
- Parsing intelligent des horaires en fran√ßais

## Cas d'Usage

### Sites E-commerce (ex: Velomania)
- Extraction automatique des fiches magasins avec horaires
- D√©tection des pages de cat√©gories de produits
- Normalisation des coordonn√©es de contact

### Sites de Services (ex: ERZA Nettoyage)
- Extraction des prestations en listes √† puces
- Structuration des FAQs
- Centralisation des coordonn√©es

### Analyse Multi-sites
- Comparaison de contenus via hash
- Statistiques par langue et type de page
- Export Markdown compatible LLM (Claude, GPT, etc.)

## D√©veloppement

Version actuelle: **2.0.0**

Technologies:
- FastAPI 0.100+
- BeautifulSoup4 4.12+
- httpx (async HTTP client)
- Python 3.11+
# MDCrawler
