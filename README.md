# Voice AI Optimized Crawler v2.1

Service FastAPI exposant un webhook `/crawl` qui prend une URL, explore en largeur (depth r√©glable), et renvoie un contenu ultra-simplifi√© et nettoy√© en Markdown, optimis√© pour la lecture par Voice AI.

## Version 2.1 - Voice AI Optimized üéôÔ∏è

**Con√ßu pour les agents vocaux** qui lisent le contenu √† haute voix et l'utilisent comme contexte pour comprendre une entreprise.

### üé≠ **NEW: JavaScript Rendering Support**
- ‚úÖ Crawls modern **React, Vue, Angular, Next.js** sites
- ‚úÖ Auto-detects SPAs and uses **Playwright** (headless browser)
- ‚úÖ Hybrid approach: fast httpx for static sites, browser for JS sites
- ‚úÖ No configuration needed - works automatically

Tested on: eratos.ch (React), and other modern SPAs ‚úÖ

### üßπ **NEW: Advanced Anti-Redundancy** (v2.1.1)
- ‚úÖ **Detects and removes** navigation menu blocks (50+ patterns)
- ‚úÖ **Eliminates** consecutive duplicate lines
- ‚úÖ **Filters** language switchers (DE, FR, IT, EN)
- ‚úÖ **Result**: 90%+ reduction in redundant content

Example: planzer.ch - from 11,613 lines ‚Üí ~800 lines ‚úÖ

### Optimisations Voice AI

**‚úÖ Supprim√© (bruit pour la voix)**:
- ‚ùå En-t√™tes de rapport statistiques
- ‚ùå Blocs YAML front-matter
- ‚ùå Marqueurs de pages (PAGE 1, PAGE 2)
- ‚ùå Ancres HTML `<a id="...">`
- ‚ùå Liens Markdown complexes
- ‚ùå Menus de navigation r√©p√©t√©s
- ‚ùå Titres marketing vides

**‚úÖ Format simplifi√©**:
- ‚úÖ Contact info en texte simple: `T√©l√©phone: +41 79 136 36 38`
- ‚úÖ Email simple: `Email: contact@example.com`
- ‚úÖ Adresse simple: `Adresse: Route des Jeunes 4, 1227 Les Acacias`
- ‚úÖ Horaires en liste claire
- ‚úÖ S√©parateurs simples (`---`) entre pages
- ‚úÖ D√©duplication automatique par hash de contenu

### Donn√©es Structur√©es
- ‚úÖ D√©tection automatique du type de page (home, store, product, category, service, faq, contact, about)
- ‚úÖ D√©tection de la langue (fr, de, it, en) via HTML, URL et contenu
- ‚úÖ Extraction et normalisation des coordonn√©es:
  - Emails avec liens `mailto:`
  - T√©l√©phones normalis√©s E.164 avec liens `tel:`
  - Adresses d√©tect√©es et normalis√©es
- ‚úÖ Horaires d'ouverture pars√©s en tableaux Markdown
- ‚úÖ URLs canoniques extraites
- ‚úÖ Hash de contenu pour d√©duplication

### Nettoyage & Normalisation
- ‚úÖ Normalisation des espaces, quotes, apostrophes
- ‚úÖ Formatage coh√©rent des devises (CHF, EUR, USD)
- ‚úÖ Capitalisation correcte des noms de villes suisses
- ‚úÖ Suppression des CTAs dupliqu√©s et slogans isol√©s
- ‚úÖ Fusion des phrases monosyllabiques isol√©es

### Rapport de Crawl Enrichi
- ‚úÖ Statistiques compl√®tes (pages, types, langues, doublons)
- ‚úÖ Version du crawler et User-Agent utilis√©
- ‚úÖ Horodatage ISO 8601
- ‚úÖ Compteurs par type (magasins, produits, cat√©gories trouv√©s)
- ‚úÖ Chaque page marqu√©e clairement (PAGE 1, PAGE 2, etc.)
- ‚úÖ URL source en pied de chaque page

## Installation

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Install Playwright browsers (for JS rendering)
python -m playwright install chromium
```

**Note**: Playwright browsers (~300MB) sont requis pour crawler les sites JavaScript modernes (React, Vue, etc.). Pour sites statiques uniquement, vous pouvez sauter cette √©tape et mettre `use_js_rendering=false`.

## Lancement

```bash
uvicorn app:app --host 0.0.0.0 --port 8080
```

### Docker (d√©ploiement ou ex√©cution serveur)

Build et run en exposant le port publiquement:

```bash
docker build -t crawler-webhook \
  -f "/Users/Eric.AELLEN/Documents/A - projets pro/crawler/1.0/Dockerfile" \
  "/Users/Eric.AELLEN/Documents/A - projets pro/crawler/1.0"

docker run --rm -p 8080:8080 --name crawler-webhook crawler-webhook
```

Vous pouvez ensuite appeler `http://<ip_serveur>:8080/crawl` depuis l'ext√©rieur.

### Render (h√©bergement manag√©)

1. Poussez ce dossier dans un repo Git (GitHub/GitLab).
2. Sur Render, cr√©ez un nouveau Web Service, connectez le repo.
3. Render d√©tectera `render.yaml` et provisionnera automatiquement.
4. Une URL publique sera fournie, ex: `https://simple-crawler-webhook.onrender.com/crawl`.

### Railway (h√©bergement manag√©)

1. Installez l'outil CLI Railway: `npm i -g @railway/cli`.
2. Depuis ce dossier: `railway init` (ou `railway up` pour cr√©er le service).
3. Railway d√©tectera `railway.toml`. Aucune config additionnelle requise.
4. Le service d√©marre avec: `uvicorn app:app --host 0.0.0.0 --port $PORT`.
5. Une URL publique sera fournie, ex: `https://<projet>-up.railway.app/crawl`.

## Utilisation

- GET:
```bash
curl --get \
  --data-urlencode "url=https://example.com" \
  --data-urlencode "depth=1" \
  --data-urlencode "max_pages=5" \
  --data-urlencode "same_domain=true" \
  --data-urlencode "use_sitemap=true" \
  --data-urlencode "timeout=15" \
  --data-urlencode "max_chars_per_page=15000" \
  --data-urlencode "rate_limit_delay=0.2" \
  http://localhost:8080/crawl
```

- POST (JSON):
```bash
curl -X POST http://localhost:8080/crawl \
  -H 'Content-Type: application/json' \
  -d '{
    "url": "https://example.com",
    "depth": 1,
    "max_pages": 5,
    "same_domain": true,
    "use_sitemap": true,
    "sitemap_url": null,
    "sitemap_max_urls": 200,
    "timeout": 15,
    "max_chars_per_page": 15000,
    "rate_limit_delay": 0.2,
    "exclude_patterns": ["/login", "\\.pdf$"]
  }'
```

R√©ponse: `text/markdown`.

## Param√®tres

- `url` (obligatoire): URL de d√©part.
- `depth` (0-5): profondeur BFS (0 = seulement la page de d√©part).
- `max_pages` (1-200): nombre maximum de pages √† extraire.
- `same_domain` (bool): restreindre au m√™me domaine.
- `timeout` (1-60s): timeout HTTP par requ√™te.
- `max_chars_per_page` (1000-200000): limite de caract√®res extraits par page.
- `rate_limit_delay` (0-5s): d√©lai entre les requ√™tes.
- `exclude_patterns` (liste de regex): URLs √† ignorer.
- `user_agent` (POST): UA custom.
- `use_sitemap` (bool): activer la d√©couverte via sitemap.
- `sitemap_url` (str|null): URL sitemap explicite (sinon robots.txt + /sitemap.xml).
- `sitemap_max_urls` (int): limite d'URLs import√©es depuis les sitemaps.
- `use_js_rendering` (bool): activer le rendu JavaScript pour les SPAs (d√©faut: true).

## Format de Sortie Voice AI

Format ultra-simple pour lecture vocale:

```markdown
# ERZA D√©m√©nagement Gen√®ve ‚Äì Experts du d√©m√©nagement

URL: https://erza.ch/

Chez ERZA D√©m√©nagement, nous accompagnons particuliers et professionnels dans toutes les √©tapes de leur d√©m√©nagement √† Gen√®ve et alentours.

Email: info@erza.ch

T√©l√©phone: +41 79 136 36 38

Adresse: Rte des Jeunes 4 Bis, 1227 Les Acacias

Horaires:
- Lundi: Ferm√©
- Mardi: 09:00-12:30, 13:30-18:30
- Mercredi: 09:00-12:30, 13:30-18:30
- Jeudi: 09:00-12:30, 13:30-18:30
- Vendredi: 09:00-12:30, 13:30-18:30
- Samedi: 09:00-17:00
- Dimanche: Ferm√©

# Des services de d√©m√©nagement professionnels √† Gen√®ve

Chez ERZA D√©m√©nagement, nous accompagnons particuliers et professionnels dans toutes les √©tapes de leur d√©m√©nagement √† Gen√®ve et alentours. Du nettoyage au garde-meubles, en passant par le d√©barras ou les interventions techniques, nous proposons des solutions compl√®tes, sur mesure et cl√© en main.

## Nos 3 formules de d√©m√©nagements

### Formule √âconomique ‚Äì L'essentiel √† prix malin

Id√©al pour les petits budgets ou les clients autonomes...

---

# Nettoyage de logement

URL: https://erza.ch/nos-services/nettoyage/

Un logement impeccable, pr√™t √† √™tre rendu ou habit√©...

[Contenu de la page suivante...]
```

## Types de Pages D√©tect√©s

Le crawler identifie automatiquement les types suivants:
- **home**: Page d'accueil
- **store**: Page de magasin/point de vente
- **product**: Fiche produit individuelle
- **category**: Page de cat√©gorie/catalogue
- **service**: Page de service/prestation
- **faq**: Page FAQ
- **contact**: Page de contact
- **about**: Page "√Ä propos"
- **page**: Page g√©n√©rique

## Langues D√©tect√©es

D√©tection automatique via:
1. Attribut HTML `lang`
2. Patterns dans l'URL (`/fr/`, `/de/`, etc.)
3. Analyse heuristique du contenu

Langues support√©es: `fr`, `de`, `it`, `en`, `unknown`

## Notes Techniques

- Extraction intelligente: titres, paragraphes, listes, blocs code, citations
- Nettoyage avanc√©: suppression scripts, styles, nav, footer, iframe, formulaires
- Normalisation hi√©rarchique des titres (√©vite les sauts de niveaux)
- D√©duplication bas√©e sur hash MD5 du contenu normalis√©
- Support des caract√®res sp√©ciaux et accents fran√ßais/allemand
- Normalisation t√©l√©phone E.164 pour Suisse (+41)
- Parsing intelligent des horaires en fran√ßais

## Cas d'Usage

### Sites E-commerce (ex: Velomania)
- Extraction automatique des fiches magasins avec horaires
- D√©tection des pages de cat√©gories de produits
- Normalisation des coordonn√©es de contact

### Sites de Services (ex: ERZA Nettoyage)
- Extraction des prestations en listes √† puces
- Structuration des FAQs
- Centralisation des coordonn√©es

### Analyse Multi-sites
- Comparaison de contenus via hash
- Statistiques par langue et type de page
- Export Markdown compatible LLM (Claude, GPT, etc.)

## D√©veloppement

Version actuelle: **2.0.0**

Technologies:
- FastAPI 0.100+
- BeautifulSoup4 4.12+
- httpx (async HTTP client)
- Python 3.11+
# MDCrawler
